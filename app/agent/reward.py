import json
import asyncio
from typing import List, Optional, Union
from openai.types.chat.chat_completion_message import ChatCompletionMessage
from openai import OpenAIError, AuthenticationError, RateLimitError, APIError
from app.llm import LLM  # å¯¼å…¥ LLM ç±»
from app.exceptions import TokenLimitExceeded
from app.logger import logger  # å‡è®¾å·²é…ç½®æ—¥å¿—
from app.prompt.reward import SYSTEM_PROMPT, EVALUATION_CRITERIA, build_evaluation_prompt
from app.schema import (TOOL_CHOICE_TYPE, AgentState, Message, ToolCall,
                        ToolChoice)
from openai import (
    OpenAI,
    APIError,
    AsyncAzureOpenAI,
    AsyncOpenAI,
    AuthenticationError,
    OpenAIError,
    RateLimitError,
)

REASONING_MODELS = ["o1", "o3-mini"]
MULTIMODAL_MODELS = [
    "gpt-4-vision-preview",
    "gpt-4o",
    "gpt-4o-mini",
    "claude-3-opus-20240229",
    "claude-3-sonnet-20240229",
    "claude-3-haiku-20240307",
]

class ContentValidator:
    def __init__(self):
        """
        åˆå§‹åŒ–éªŒè¯å™¨ç±»ï¼ˆéœ€æ³¨å…¥ LLM å®ä¾‹ï¼‰
        :param llm: LLM å®ä¾‹ï¼Œç”¨äºè°ƒç”¨å¤§è¯­è¨€æ¨¡å‹æ¥å£
        """
        self.global_score = 0
        self.system_prompt = SYSTEM_PROMPT

    async def final_trajectory_score(
        self,
        user_query,
        messages
    ) -> int:
        """
        æ£€éªŒ manus ç”Ÿæˆçš„æœ€ç»ˆæ–¹æ¡ˆå¹¶æ‰“åˆ†ï¼ˆè°ƒç”¨ LLM è¾…åŠ©è¯„åˆ†ï¼‰
        :return: åˆç†æ€§å¾—åˆ†
        """
        # è°ƒç”¨ LLM è·å–è¯„åˆ†å“åº”
        llm_response = await self.ask_reward(query = user_query,
                    messages=messages,
                    system_msgs =
                    self.system_prompt
                    if self.system_prompt
                    else None
                )
        logger.info(f'ğŸ‘€ğŸ‘€ è°ƒç”¨æ¨¡å‹è¯„ä¼°æœ€ç»ˆçš„è¾“å‡º: {llm_response}')
        total = json.loads(llm_response)["weighted_total"]

        if not llm_response:
            return 0  # æˆ–æ ¹æ®éœ€æ±‚å¤„ç†å¼‚å¸¸æƒ…å†µ

        # å‡è®¾ LLM å“åº”å†…å®¹ä¸­åŒ…å«è¯„åˆ†å­—æ®µ
        # try:
        #     score = int(llm_response.choices)
        #     self.global_score += score
        #     return score
        # except (ValueError, IndexError):
        #     logger.error("LLM å“åº”æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æè¯„åˆ†")
        return total

    async def ask_reward(
        self,
        query,
        messages,
        system_msgs,
        timeout: int = 300,
        temperature: int = 0,
        **kwargs,
    ) -> ChatCompletionMessage | None:
        """
        è°ƒç”¨ LLM ç»™æœ€ç»ˆæ–¹æ¡ˆæ‰“åˆ†ï¼ˆæ•´åˆåçš„æ–¹æ³•ï¼‰
        """

        # openai_api_key = "EMPTY"
        # openai_api_base = "http://10.66.180.89:30000/v1"
        # model = "/nfs/ofs-llab-cold/model/deepseek-ai/DeepSeek-R1-0528"

        openai_api_key = "9cb0a12b-dd0c-4a0f-a26c-47f799eab8ff"
        openai_api_base = "https://ark.cn-beijing.volces.com/api/v3"
        model = "deepseek-r1-250528"

        client = AsyncOpenAI(
            api_key=openai_api_key,
            base_url=openai_api_base,
        )

        try:
            # æ ¼å¼åŒ–æ¶ˆæ¯ï¼ˆé€šè¿‡æ³¨å…¥çš„ llm å®ä¾‹è°ƒç”¨ï¼‰
            final_msg = build_evaluation_prompt(messages, EVALUATION_CRITERIA)

            user_query = 'ç”¨æˆ·éœ€æ±‚: ' + query
            system_msgs = system_msgs + "\n" + user_query

            reward_messages = [
                # ç³»ç»Ÿæ¶ˆæ¯ï¼ˆè®¾å®šè§’è‰²å’Œä»»åŠ¡ï¼‰
                {"role": "system", "content": system_msgs},

                # ç”¨æˆ·å½“å‰é—®é¢˜
                {"role": "user", "content": final_msg}
            ]

            # æ„é€ è¯·æ±‚å‚æ•°
            response: ChatCompletion = await client.chat.completions.create(
            model=model,
            messages=reward_messages,
            temperature=temperature,
            timeout=timeout,
            response_format={"type": "json_object"}
            )

            # logger.info(f'ğŸ‘€ğŸ‘€ è°ƒç”¨æ¨¡å‹è¯„ä¼°æœ€ç»ˆçš„è¾“å‡º: {response.choices[0].message.content}')
            # print(type(response.choices[0].message.content))
            # print(response.choices[0].message.content["weighted_total"])
            # æ£€æŸ¥å“åº”æœ‰æ•ˆæ€§
            # if not response.choices or not response.choices[0].message:
            #     logger.error("LLM è¿”å›ç©ºå“åº”")
            #     return None

            return response.choices[0].message.content

        except TokenLimitExceeded:
            raise  # ä¸è®°å½•æ—¥å¿—ï¼Œç›´æ¥æŠ›å‡º
        except ValueError as ve:
            logger.error(f"ask_reward éªŒè¯é”™è¯¯: {ve}")
            raise
        except OpenAIError as oe:
            logger.error(f"OpenAI API é”™è¯¯: {oe}")
            if isinstance(oe, AuthenticationError):
                logger.error("è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API key")
            elif isinstance(oe, RateLimitError):
                logger.error("é€Ÿç‡é™åˆ¶è¶…é™ï¼Œå°è¯•å¢åŠ é‡è¯•æ¬¡æ•°")
            elif isinstance(oe, APIError):
                logger.error(f"API é”™è¯¯è¯¦æƒ…: {oe}")
            raise
        except Exception as e:
            logger.error(f"ask_reward æ„å¤–é”™è¯¯: {e}")
            raise

    # å…¶ä»–è¾…åŠ©æ–¹æ³•ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
    def _check_specific_condition(self, content):
        """
        æ£€æŸ¥ç‰¹å®šæ¡ä»¶
        :return: å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦æ»¡è¶³æ¡ä»¶
        """
        pass


if __name__ == "__main__":
    from app.llm import LLM
    from app.config import LLMSettings, config
    from app.schema import Message

    # åˆå§‹åŒ– LLM å®ä¾‹
    llm_config = LLMSettings()  # å‡è®¾ LLMSettings å¯ä»¥æ— å‚æ•°åˆå§‹åŒ–
    llm = LLM(config_name="default", llm_config=llm_config)

    # åˆå§‹åŒ– ContentValidator å®ä¾‹
    validator = ContentValidator(llm)

    # æ¨¡æ‹Ÿæ¶ˆæ¯åˆ—è¡¨
    test_messages = [
        Message(user="user", content="è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯")
    ]

    try:
        # è°ƒç”¨å¼‚æ­¥çš„ final_trajectory_score æ–¹æ³•
        import asyncio
        score = asyncio.run(validator.final_trajectory_score(test_messages))
        print(f"æœ€ç»ˆæ–¹æ¡ˆçš„åˆç†æ€§å¾—åˆ†: {score}")
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
